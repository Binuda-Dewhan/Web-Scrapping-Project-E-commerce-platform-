# ğŸ›ï¸ E-Commerce Analytics Automation Project

## ğŸ“Š Overview

This project automates the scraping and analysis of laptop product data and customer reviews from a major e-commerce platform (e.g., BestBuy.com). It includes:
- Data scraping with Selenium
- Sentiment analysis with TextBlob
- Excel report generation with openpyxl
- Modular utility scripts
- Structured logs and reusable pipelines

---

## ğŸš€ Features

- âœ… Filter laptops by price ($500â€“$1500), brand (HP, Dell, Lenovo), and rating (4â˜…+)
- âœ… Collect and store product listings and full product details in JSON
- âœ… Extract customer reviews across paginated pages
- âœ… Perform sentiment analysis on reviews (Positive, Neutral, Negative)
- âœ… Generate Excel reports with:
  - Conditional formatting
  - Pivot tables and filters
  - Comparison matrix of specs
  - Word clouds and sentiment visualizations
- âœ… Logging and modular architecture
- âœ… Configuration using `.env` and `config.py`

---

## ğŸ“¦ Folder Structure

```
ecommerce-analytics/
â”œâ”€â”€ .env                      # Environment config
â”œâ”€â”€ config.py                 # Loads values from .env
â”œâ”€â”€ main.py                   # Controls scraping flow
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ data_processor.py     # Cleans + analyzes data, generates Excel & visuals
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                  # Contains raw JSON files per product
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ analysis.log          # All logs stored here
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ product_analysis.xlsx # Final Excel output
â”‚   â”œâ”€â”€ all_wordcloud.png
â”‚   â”œâ”€â”€ positive_wordcloud.png
â”‚   â”œâ”€â”€ negative_wordcloud.png
â”‚   â””â”€â”€ sentiment_distribution.png
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ category_scraper.py   # Extracts laptops from main category
â”‚   â””â”€â”€ product_scraper.py    # Extracts full details + reviews per product
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ delay_utils.py
â”‚   â”œâ”€â”€ json_utils.py
â”‚   â”œâ”€â”€ wait_utils.py
â”‚   â””â”€â”€ browser_manager.py
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # (This file)
```

---

## ğŸ› ï¸ Step-by-Step Breakdown

### âœ… Step 1: `.env` Configuration
- Centralized configuration using `.env` for timeout, user-agent, etc.

**Example `.env`:**
```ini
PAGE_LOAD_TIMEOUT=15
USER_AGENT=Mozilla/5.0 (...)
```

## âœ… Step 2: `config.py`

Reads values from `.env` using `python-dotenv`.

```python
from dotenv import load_dotenv
import os

load_dotenv()

PAGE_LOAD_TIMEOUT = int(os.getenv("PAGE_LOAD_TIMEOUT", 15))
USER_AGENT = os.getenv("USER_AGENT", "default-user-agent")
```

---

## âœ… Step 3: `category_scraper.py`

Scrapes laptops with applied filters via URL.

- Extracts product name, price, rating, review count, short specs, and product URL  
- Uses scrolling and utility delays

---

## âœ… Step 4: `product_scraper.py`

Reads scraped product URLs and visits each product page to collect:

- Full technical specs  
- Paginated customer reviews  
- Updates JSON with detailed info

---

## âœ… Step 5: `main.py`

Acts as the orchestrator:

- Calls both scrapers  
- Manages logging and flow

## âœ… Step 6: `data_processor.py`

Processes raw JSON files to generate an Excel workbook:

### ğŸ“˜ Sheet 1: Product Summary
- Basic product info  
- Conditional formatting on price  
- Dropdown filters by brand  
- Excel table layout

### ğŸ“˜ Sheet 2: Specification Comparison
- Matrix format for specs  
- Highlights:
  - Max RAM, battery â†’ ğŸŸ© Green  
  - Min weight â†’ ğŸŸ¦ Blue

### ğŸ“˜ Sheet 3: Review Analysis
- Sentiment analysis (using `TextBlob`)  
- Sentiment-labeled reviews  
- Generates:
  - Word clouds (All / Positive / Negative)  
  - Sentiment distribution chart  
  - CSV of labeled reviews

---

## ğŸ”§ Utility Modules

| File               | Purpose                           |
|--------------------|-----------------------------------|
| `wait_utils.py`    | Explicit wait handling            |
| `delay_utils.py`   | Adds random delay between actions |
| `json_utils.py`    | Save/load/update JSON             |
| `browser_manager.py` | Chrome browser setup            |

---

## ğŸªµ Logging

- All operations are logged to `logs/analysis.log`  
- Helpful for debugging and status monitoring

---

## ğŸ§ª Testing

Basic unit tests are included in the `tests/` directory.

To run them:

```bash
pytest tests/
```

---

## ğŸ“‹ Requirements

Install all dependencies:

```bash
pip install -r requirements.txt
```

**Main packages used:**
- `selenium`  
- `pandas`  
- `openpyxl`  
- `nltk`  
- `matplotlib`, `seaborn`  
- `python-dotenv`

---

## ğŸ“Œ Notes

- The scraper and analyzer are modular â€“ easily extendable to other categories.  
- You can plug in more advanced NLP or price alerting if needed.

---

## ğŸ™‹ Author

**Binuda Dewhan Bandara**  
Feel free to connect or raise issues in this repo!

---

## ğŸ“œ License

This project is for academic and demo purposes only.

---

### âœ… To update your current `README.md` file:

1. Open the file in any code editor (VS Code, Notepad++, etc.)
2. Replace all existing content with the markdown above
3. Save the file
4. Commit and push:

```bash
git add README.md
git commit -m "Update README with full project description"
git push
```


