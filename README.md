Step 1: .env Configuration
- Create a .env file to store environment-specific configurations (e.g., timeouts, user-agent, paths).
- This allows for centralized configuration and easier tuning without editing source code.

Example .env:
PAGE_LOAD_TIMEOUT=15
USER_AGENT=Mozilla/5.0 (...)

Step 2: config.py
- Read values from the .env file using dotenv.
- Define constants like page load timeout and user agent string.

Example:
from dotenv import load_dotenv
import os

load_dotenv()

PAGE_LOAD_TIMEOUT = int(os.getenv("PAGE_LOAD_TIMEOUT", 15))
USER_AGENT = os.getenv("USER_AGENT", "default-user-agent")

Step 3: category_scraper.py â€“ Scraping Laptop Listings
- Navigates to the BestBuy Laptops category page.
- Filters are already applied via URL:
  - Price: $500â€“$1500
  - Brands: HP, Dell, Lenovo
  - Customer Rating: 4 stars & up
- Extracts:
  - Product Name
  - Price
  - Rating
  - Review Count
  - Short Specifications
  - Product URL
- Uses utility functions for delays, waits, and saving JSON.
- Supports lazy-loading product cards by scrolling the page.

Step 4: product_scraper.py â€“ Advanced Scraping per Product
- Loads individual JSON files from the previous step.
- Opens each product URL to extract:
  - Full technical specifications (as a dictionary)
  - All customer reviews (supports pagination)
- Appends this data to the corresponding JSON file using update_product_json.

Step 5: main.py
- Central coordinator for scraping.
- Calls:
  - LaptopCategoryScraper to extract listings
  - ProductDetailScraper to extract full specs + reviews
- Controls flow and logging.

Step 6: data_processor.py â€“ Data Cleaning & Analysis
Processes all collected JSON files to generate reports.

 Creates product_analysis.xlsx with the following sheets:

 Sheet 1: Product Summary
- Basic data: name, price, rating, brand, specs
- Conditional formatting on price (color-coded)
- Dropdown filtering by brand
- Excel table for clean visualization

 Sheet 2: Specifications Comparison
- Matrix of technical specs (RAM, storage, CPU, screen, battery, etc.)
- Highlights best-in-class features:
  - Max RAM (green)
  - Max Battery (green)
  - Min Weight (blue)
- Excel Table for sortable comparisons

 Sheet 3: Review Analysis
- Performs sentiment analysis using TextBlob
- Labels reviews as Positive, Neutral, or Negative
- Saves results into Excel
- Also generates:
  - Word Clouds (for All / Positive / Negative reviews)
  - Sentiment score distribution chart
  - CSV of labeled reviews for future analysis

Utility Files
These are helper modules used throughout the scraping and analysis process:
- wait_utils.py â€“ Explicit wait handling using WebDriverWait
- delay_utils.py â€“ Applies random delays (to prevent blocking)
- json_utils.py â€“ Save/load/update product JSON files
- browser_manager.py â€“ Manages Selenium Chrome browser setup

Logs
- Logging is implemented throughout for debugging and traceability.
- All logs are saved to logs/analysis.log.

ğŸ“¦ Folder Structure
ecommerce-analytics/
â”œâ”€â”€ .env                         # Environment config
â”œâ”€â”€ config.py                    # Loads values from .env
â”œâ”€â”€ main.py                      # Controls scraping flow
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ data_processor.py        # Cleans + analyzes data, generates Excel & visuals
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                     # Contains raw JSON files per product
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ analysis.log             # All logs stored here
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ product_analysis.xlsx    # Final Excel output
â”‚   â”œâ”€â”€ all_wordcloud.png
â”‚   â”œâ”€â”€ positive_wordcloud.png
â”‚   â”œâ”€â”€ negative_wordcloud.png
â”‚   â””â”€â”€ sentiment_distribution.png
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ category_scraper.py      # Extracts laptops from main category
â”‚   â””â”€â”€ product_scraper.py       # Extracts full details + reviews per product
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ delay_utils.py
â”‚   â”œâ”€â”€ json_utils.py
â”‚   â”œâ”€â”€ wait_utils.py
â”‚   â””â”€â”€ browser_manager.py
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.txt                   # (This file)

ğŸ“ Notes
- This project is modular, so you can reuse the scraper for other product categories.
- You can expand data_processor.py to include more advanced NLP, trends, or pricing alerts.
- All Excel formatting is handled using openpyxl.
